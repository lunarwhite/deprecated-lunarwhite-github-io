<!doctype html><html><head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge"><title>Deeplearning introduction - Ⅰ - lunarwhite</title><link rel=icon type=image/png href=favicon/favicon.ico><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Brief and customised manuals for me to check.">
<meta property="og:image" content>
<meta property="og:title" content="Deeplearning introduction - Ⅰ">
<meta property="og:description" content="Brief and customised manuals for me to check.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://lunarwhite.github.io/posts/guides/dl-intro1/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-06-24T20:20:48+00:00">
<meta property="article:modified_time" content="2021-06-24T20:20:48+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Deeplearning introduction - Ⅰ">
<meta name=twitter:description content="Brief and customised manuals for me to check.">
<script src=https://lunarwhite.github.iojs/feather.min.js></script>
<link href=https://lunarwhite.github.io/css/fonts.b685ac6f654695232de7b82a9143a46f9e049c8e3af3a21d9737b01f4be211d1.css rel=stylesheet>
<link rel=stylesheet type=text/css media=screen href=https://lunarwhite.github.io/css/main.2cd813e50a99e0d45695e178eb612f3c1b609d0213e57cffe91396333316436e.css>
</head>
<body>
<div class=content><header>
<div class=main>
<a href=https://lunarwhite.github.io>lunarwhite</a>
</div>
<nav>
<a href=/>Home</a>
<a href=/posts>Posts</a>
<a href=/tags>Tags</a>
<a href=/about>About</a>
</nav>
</header>
<main>
<article>
<div class=title>
<h1 class=title>Deeplearning introduction - Ⅰ</h1>
<div class=meta>Posted on Jun 24, 2021</div>
</div>
<section class=body>
<h2 id=1-简介>1 简介</h2>
<h3 id=11-一种定义>1.1 一种定义</h3>
<ul>
<li>P（Performance）来评估计算机程序在某类任务T（Task）上的性能，若一个程序通过利用经验E（Experience）在T中任务上获得了性能改善，则我们就说关于T和P，该程序对E进行了学习</li>
</ul>
<h3 id=12-一种划分>1.2 一种划分</h3>
<ul>
<li>分类Classification
<ul>
<li>输出值离散</li>
<li>e.g. 根据光谱的形状，分类为恒星、星系或者其它</li>
</ul>
</li>
<li>回归Regression
<ul>
<li>输出连续值</li>
<li>e.g. 根据光谱的形状，估计恒星的温度</li>
</ul>
</li>
<li>聚类Clustering
<ul>
<li>把相似的记录聚在一起，不需要标签</li>
<li>e.g. 对获得天体光谱聚类，相似的光谱被聚在同一组</li>
</ul>
</li>
</ul>
<h3 id=13-三要素>1.3 三要素</h3>
<ul>
<li>数据、算法和模型</li>
</ul>
<h3 id=14-大致步骤>1.4 大致步骤</h3>
<ul>
<li>1 Define a Model</li>
<li>2 Goodness of Function</li>
<li>3 Pick the Best Function: Gradient decent</li>
</ul>
<h3 id=15-难点>1.5 难点</h3>
<ul>
<li>调用算法得出了准确率？</li>
<li>模型搭建过程以及评价指标算的是对的？</li>
<li>模型选择，并且能说明模型选的合适？</li>
<li>评价指标是不是符合真正的需求？</li>
<li>训练集是否覆盖全面？</li>
<li>数据里面有没有错误值、错误标签影响到了结果？</li>
<li>获得的模型能真正用于实际应用？</li>
<li>把过程进行清楚、有逻辑的表达，展示给别人？</li>
</ul>
<h3 id=16-发展>1.6 发展</h3>
<ul>
<li>深层网络训练中，梯度消失问题
<ul>
<li>方法：无监督预训练对权值进行初始化+有监督训练微调</li>
<li>ReLU激活函数被提出，能够有效的抑制梯度消失问题</li>
</ul>
</li>
<li>Loss的局部极值问题
<ul>
<li>对于深层网络来说影响可以忽略</li>
<li>原因：深层网络虽然局部极值非常多，但是通过DL的BatchGradientDescent优化方法很难陷进去，而且就算陷进去，其局部极小值点与全局极小值点也是非常接近，但是浅层网络却不然，其拥有较少的局部极小值点，但是却很容易陷进去，且这些局部极小值点与全局极小值点相差较大</li>
</ul>
</li>
</ul>
<h3 id=17-一些概念>1.7 一些概念</h3>
<ul>
<li>softmax
<ul>
<li>将预测结果转化为非负数，各种预测结果概率之和等于1</li>
</ul>
</li>
<li>one-hot
<ul>
<li>将离散型特征使用one-hot编码，让特征之间的距离计算更加合理</li>
</ul>
</li>
</ul>
<h2 id=2-线性回归>2 线性回归</h2>
<h3 id=21-模型>2.1 模型</h3>
<ul>
<li>公式：$y'=b+wx$，标签=偏差+权重x特征，通过训练确定w和b</li>
</ul>
<h3 id=22-训练与损失>2.2 训练与损失</h3>
<ul>
<li>如何找出最优的函数 ？1.需要对函数评价
<ul>
<li>Loss：对单个样本而言是一个数值，反应了模型预测的准确程度；如果model的预测完全准确，则损失为零</li>
<li>Loss-Function：
<ul>
<li>经验风险最小化：监督学习中算法检查多个样本，尝试找出Loss最小的model</li>
<li>结构风险最小化：防止模型过于复杂，加入惩罚项</li>
</ul>
</li>
<li>训练目标：从所有样本中找到一组平均Loss较小的权重w和偏差b</li>
</ul>
</li>
<li>如何找出最优的函数 ？2.迭代方法降低损失
<ul>
<li>训练模型通常随机给出一组参数，学习过程通常不断迭代，直到总体损失不再变化或至少变化极其缓慢为止，该模型已收敛</li>
</ul>
</li>
</ul>
<h3 id=23-降低损失梯度下降>2.3 降低损失：梯度下降</h3>
<ul>
<li>梯度
<ul>
<li>是偏导数的矢量；具有以下两个特征：方向和大小</li>
<li>大小：损失Loss相对于单个权重的导数</li>
<li>方向：指向损失函数中增长最快的方向</li>
</ul>
</li>
<li>梯度下降
<ul>
<li>$w1=w1-lr*w1_{grad}$</li>
<li>一种寻找目标函数最小化的方法，沿梯度的方向不一定是最优的方向</li>
<li>当目标函数是凸函数时，梯度下降法的解是全局最优</li>
</ul>
</li>
</ul>
<h3 id=24-降低损失学习速率>2.4 降低损失：学习速率</h3>
<ul>
<li>大小选取
<ul>
<li>过小，花费太长的学习时间</li>
<li>过大，下一个点将永远在U形曲线的底部随意弹跳</li>
</ul>
</li>
</ul>
<h2 id=3-过拟合>3 过拟合</h2>
<h3 id=31-概述>3.1 概述</h3>
<ul>
<li>表现：训练误差很小，测试误差却很大</li>
<li>原因：模型的复杂程度超出所需程度</li>
</ul>
<h3 id=32-泛化能力>3.2 泛化能力</h3>
<ul>
<li>是什么：学习到的模型对未知数据的预测能力</li>
<li>如何评判：用测试误差来评价模型的泛化能力</li>
</ul>
<h3 id=33-验证集>3.3 验证集</h3>
<ul>
<li>不要对测试数据进行训练
<ul>
<li>如果评估指标取得了意外的好结果，有可能是因为测试数据和训练数据有重叠，无法准确衡量该模型泛化到新数据的效果</li>
</ul>
</li>
<li>划分方法
<ul>
<li>划分为三个子集，大幅降低过拟合的发生几率</li>
</ul>
</li>
<li>训练方法
<ul>
<li>训练集训练，验证集评估效果</li>
<li>——>不断调整模型，选择最佳效果模型</li>
<li>——>使用测试集，确认模型效果</li>
</ul>
</li>
</ul>
<h3 id=34-方法>3.4 方法</h3>
<ul>
<li>更多的训练数据</li>
<li>划分验证集</li>
<li>L1，L2正则化</li>
<li>减小模型复杂度，特征选择</li>
</ul>
<h2 id=4-梯度下降-tips>4 梯度下降-tips</h2>
<h3 id=40-原因>4.0 原因</h3>
<ul>
<li>在假设的复杂函数中引入了大量的非线性变换，损失函数很难是凸的</li>
<li>两个挑战：局部极小值、鞍点</li>
</ul>
<h3 id=41-sgd随机梯度下降>4.1 SGD随机梯度下降</h3>
<ul>
<li>特点
<ul>
<li>在每次迭代时使用一个样本来对参数进行更新</li>
<li>搜索看起来很盲目（之字形），根本原因梯度方向没有指向最低点</li>
</ul>
</li>
<li>缺点
<ul>
<li>迭代次数，下降速度慢</li>
<li>容易陷入震荡，可能停留在局部最优点</li>
<li>学习率不好确定</li>
</ul>
</li>
</ul>
<h3 id=43-sgd-momentum>4.3 SGD-Momentum</h3>
<ul>
<li>特点
<ul>
<li>为了加速SGD收敛并抑制震荡，加入动量Momentum</li>
<li>动量法的超参数为：学习率、动量参数，学习率不变</li>
<li>下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些</li>
</ul>
</li>
<li>原因
<ul>
<li>使得参数中那些梯度方向变化不大的维度可以加速更新</li>
<li>并减少梯度方向变化较大的维度上的更新幅度</li>
</ul>
</li>
</ul>
<h3 id=44-bgd批量梯度下降>4.4 BGD批量梯度下降</h3>
<ul>
<li>特点
<ul>
<li>一次迭代对所有样本进行梯度的更新，迭代的次数相对较少</li>
<li>利用向量化进行操作，实现了并行</li>
</ul>
</li>
<li>优点
<ul>
<li>使用固定学习率，不担心学习率衰退</li>
<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确朝向极值所在方向</li>
<li>当目标函数为凸，一定能收敛到全局最小值；如果非凸则，收敛到局部最小值</li>
</ul>
</li>
<li>缺点
<ul>
<li>遍历全部样本仍需要大量时间</li>
<li>每次更新都在遍历全部样例之后，无法提前发现多余的例子</li>
</ul>
</li>
</ul>
<h3 id=45-mini-bgd>4.5 mini-BGD</h3>
<ul>
<li>特点
<ul>
<li>默认人们说的BGD就是mini-BGD</li>
<li>BGD与SGD之间的折衷方案，包含10-1000个随机选择的样本</li>
<li>可以减少杂乱，比BGD更高效</li>
</ul>
</li>
<li>关键参数
<ul>
<li>学习率：有必要随着时间的推移逐渐降低学习率，目标函数能够进一步降低，有助于算法的收敛，更容易接近最优解</li>
<li>Batch-Size：一般来说越大，其确定的下降方向越准，引起训练震荡越小，跑完一次 epoch（全数据集）所需的迭代次数减少，相同数据量的处理速度加快，</li>
</ul>
</li>
<li>优点
<ul>
<li>计算速度比BGD快，因为只遍历部分样例就可执行更新</li>
<li>随机选择样例，摆脱局部最小值</li>
<li>随机选择样例，避免重复的样例、对参数更新较少贡献的样例</li>
<li>减小收敛所需要的迭代次数，使收敛到的结果更加接近梯度下降的效果</li>
</ul>
</li>
<li>缺点
<ul>
<li>需要增加学习率衰减项，以降低学习率，避免过度振荡</li>
<li>Batch-Size选择不当带来问题</li>
</ul>
</li>
</ul>
<h3 id=46-adagrad>4.6 Adagrad</h3>
<ul>
<li>特点
<ul>
<li>SGD、SGD-Momentum均是以相同的学习率去更新theta的各个分量。而深度学习模型中往往涉及大量的参数，不同参数的更新频率往往有所区别：对更新不频繁的参数，希望单次步长大，多学习一些知识；对更新频繁的参数，希望单次步长小，使学习到的参数更稳定，被单个样本影响太多</li>
<li>引入了二阶动量，学习率衰减，为不同的变量提供不同的学习率：对频繁更新过的参数，其二阶动量的对应分量较大，学习率较小；学习越深入，更新幅度会越来越小</li>
</ul>
</li>
<li>缺点
<ul>
<li>学习率逐渐递减至 0，可能导致训练过程提前结束</li>
</ul>
</li>
</ul>
<h3 id=47-rmsprop>4.7 RMSprop</h3>
<ul>
<li>特点
<ul>
<li>遗忘部分历史梯度版本的AdaGrad</li>
<li>逐渐遗忘过去的梯度，保留新梯度信息</li>
</ul>
</li>
</ul>
<h3 id=48-adam>4.8 Adam</h3>
<ul>
<li>特点
<ul>
<li>SGD-Momentum在SGD基础上增加了一阶动量</li>
<li>AdaGrad在SGD基础上增加了二阶动量</li>
<li>把一阶动量和二阶动量都用起来，就是Adam</li>
</ul>
</li>
</ul>
<h3 id=49-可视化>4.9 可视化</h3>
<ul>
<li>总结
<ul>
<li>Adagrad、RMSprop从最开始就找到了正确的方向并快速收敛</li>
<li>SGD找到了正确方向但收敛速度很慢</li>
<li>SGD-Momentum最初偏离了航道，但也能最终纠正到正确方向</li>
</ul>
</li>
</ul>
<figure><img src=/images/dl-into1/01.png>
</figure>
<figure><img src=/images/dl-into1/02.gif>
</figure>
<figure><img src=/images/dl-into1/03.gif>
</figure>
<figure><img src=/images/dl-into1/04.gif>
</figure>
<h2 id=5-模型的有效性>5 模型的有效性</h2>
<h3 id=50-分类阈值>5.0 分类阈值</h3>
<ul>
<li>为了将逻辑回归值映射到二元类别，指定分类阈值</li>
<li>如果值高于该阈值，表示垃圾；低于该阈值，表示非垃圾</li>
</ul>
<h3 id=51-四个类别>5.1 四个类别</h3>
<ul>
<li>真正例：模型将正类别样本&lt;正确地预测>为正类别。True Positive,TP</li>
<li>真负例：模型将负类别样本&lt;正确地预测>为负类别。True Negative,TN</li>
<li>假正例：模型将负类别样本&lt;错误地预测>为正类别。False Positive,FP</li>
<li>假负例：模型将正类别样本&lt;错误地预测>为负类别。 False Negative,FN</li>
</ul>
<h3 id=52-评价指标>5.2 评价指标</h3>
<ul>
<li>准确率：(预测正确)/(all)
<ul>
<li>缺陷：使用分类不平衡的数据集（数量明显差异），单准确率一项并不能反映全面情况</li>
</ul>
</li>
<li>精确率（查准率）：(TP)/(预测为P)</li>
<li>召回率（查全率）：(TP)/(真正为P)
<ul>
<li>特点：精确率和召回率此消彼长，提高精确率通常会降低召回率值</li>
</ul>
</li>
<li>邮件分类问题，垃圾邮件为正例，提高分类阈值
<ul>
<li>精确率：不可能会降低</li>
<li>召回率：不可能会会升高</li>
</ul>
</li>
<li>ROC（接收者操作特征）
<ul>
<li>曲线上每个点反映着对同一信号刺激的感受性</li>
<li>评判分类器的效果，不受分类阈值的影响，对不平衡数据也有效</li>
<li>是关于不同系统或绘制的性能点之间的权衡，而不是单个系统的性能</li>
<li>找出最优的分类阈值——>面积</li>
</ul>
</li>
<li>画ROC曲线
<ul>
<li>TPR=(TP)/(真正为P)；FPR=(FP)/(真正为F)</li>
<li>从高到低依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本</li>
<li>从第一个样本开始，设该样本的Score值为阈值，则该样本及之后的样本(均比该样本概率值小)判为负样本，即所有样本判为全负，计算得TPR=FPR=0，即ROC曲线(0,0)点</li>
<li>再选第二个样本点的Score作为阈值，大于等于该阈值的样本判为正样本，小于该阈值的判为负样本，计算TPR和FRP，可在ROC图画出该点</li>
</ul>
</li>
<li>AUC（ROC 曲线下面积）
<ul>
<li>有时ROC曲线并不能清晰的说明哪个分类器的效果更好</li>
<li>引入AUC作为一个数值，对所有可能的分类阈值的效果进行综合衡量，越大越好</li>
</ul>
</li>
</ul>
<h2 id=6-误差分析>6 误差分析</h2>
<h3 id=60-error>6.0 Error</h3>
<ul>
<li>Error=Bias +Variance</li>
<li>射击时，瞄的准（bias小），手不抖（variance小）</li>
</ul>
<figure><img src=/images/dl-into1/05.png>
</figure>
<h3 id=61-bias>6.1 Bias</h3>
<ul>
<li>Bias大：欠拟合</li>
<li>怎么办：重新设计模型：1）增加特征2）增加模型复杂度</li>
</ul>
<h3 id=62-variance>6.2 Variance</h3>
<ul>
<li>Variance大：过拟合</li>
<li>怎么办：1）更多data；2）正则化</li>
</ul>
<h3 id=63-模型选择>6.3 模型选择</h3>
<ul>
<li>交叉验证，n折交叉验证，选择在验证集平均Error最小的模型</li>
</ul>
<h2 id=8-dnn-tips>8 DNN-tips</h2>
<h3 id=81-解决没学好的问题>8.1 解决没学好的问题</h3>
<ul>
<li>梯度消失
<ul>
<li>1）出现在深层网络中</li>
<li>2）采用了不合适的损失函数，比如sigmoid</li>
</ul>
</li>
<li>梯度爆炸
<ul>
<li>1）出现在深层网络中</li>
<li>2）权值初始化值太大</li>
</ul>
</li>
<li>解决方案
<ul>
<li>预训练加微调</li>
<li>梯度剪切：设置梯度剪切阈值，更新梯度时如果超过，那就强制限制在这个范围内</li>
<li>激活函数：ReLU（如果激活函数的导数在正数部分是恒等于1，那么就不存在梯度消失爆炸的问题，每层的网络都可以得到相同的更新速度）、LeakReLU等
<ul>
<li>$ReLU(x)=max(x, 0)$
<ul>
<li>优点：解决了梯度消失、爆炸，计算速度快，加速训练</li>
<li>缺点：由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）；输出不是以0为中心的</li>
</ul>
</li>
<li>$leakrelu=max(k*x, x)$
<ul>
<li>就是为了解决relu的0区间带来的影响</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure><img src=/images/dl-into1/06.png>
</figure>
<ul>
<li>MAXOUT：ReLU就是其中一种</li>
<li>自适应学习率：RMSProp、Momentum和Adam</li>
<li>Batch-Norm：加速网络收敛速度，提升训练稳定性</li>
<li>LSTM长短期记忆网络：不那么容易发生梯度消失，主要因为LSTM通过内部复杂的门，可以接下来更新的时候记住前几次训练的残留记忆</li>
</ul>
<h3 id=82-解决过拟合问题>8.2 解决过拟合问题</h3>
<ul>
<li>解决方案
<ul>
<li>Early-Stopping，直到损失不再降低</li>
</ul>
</li>
</ul>
<figure><img src=/images/dl-into1/07.png>
</figure>
<ul>
<li>L1 L2正则化
<ul>
<li>L2正则化（权重衰减）的目的：让权重衰减到更小的值，一定程度减少过拟合</li>
</ul>
</li>
<li>Dropout</li>
</ul>
<h2 id=9-cnn>9 CNN</h2>
<h3 id=91-简介>9.1 简介</h3>
<ul>
<li>CNN的精髓：
<ul>
<li>局部连接、权值共享，减小计算量</li>
</ul>
</li>
<li>CNN和普通DNN不同：
<ul>
<li>并不需要为每个神经元所对应的每个输入数据提供单独的权重</li>
<li>与池化相结合，CNN可以被理解为一种公共特征的提取过程</li>
<li>不仅是CNN，大部分神经网络都可以近似的认为大部分神经元都被用于特征提取</li>
</ul>
</li>
<li>反向传播算法，确定参数（权重）
<ul>
<li>前向传播、损失函数、反向传播、权重更新</li>
</ul>
</li>
</ul>
<h3 id=92-过程>9.2 过程</h3>
<ul>
<li>填充 Padding
<ul>
<li>让height和width下降的不那么快，有时候需要大小不变</li>
<li>更好的保留图像边缘的信息</li>
</ul>
</li>
<li>卷积步长 Stride</li>
<li>卷积核 Filter
<ul>
<li>多个核，提取不同的特征</li>
<li>卷积核越多，激活图的深度越深，得到关于输入图像的信息越多</li>
</ul>
</li>
<li>最大池化 Max-Pooling
<ul>
<li>保留了每一个小块内的最大值，相当于保留了这一块最佳匹配结果</li>
</ul>
</li>
<li>示例</li>
</ul>
<figure><img src=/images/dl-into1/08.jpg>
</figure>
<figure><img src=/images/dl-into1/10.png>
</figure>
<ul>
<li>计算：
<ul>
<li>输入图片：$n*n*n_c$</li>
<li>单个卷积核：$f*f*n_c$</li>
<li>卷积核个数：$k$</li>
<li>padding：$p$</li>
<li>stride：$s$</li>
<li>结果：$(⌊\frac{n+2p-f}{s}+1⌋)∗(⌊\frac{n+2p-f}{s}+1⌋)∗k$</li>
</ul>
</li>
</ul>
</section>
<div class=post-tags>
<nav class="nav tags">
<ul class=tags>
<li><a href=/tags/deep-learning>deep-learning</a></li>
<li><a href=/tags/self-tutorial>self-tutorial</a></li>
</ul>
</nav>
</div>
</article>
</main>
<footer>
<hr><a class=soc href=https://www.linkedin.com/in/yuedongwoo/ title=LinkedIn><i data-feather=linkedin></i></a>|<a class=soc href=https://github.com/lunarwhite title=GitHub><i data-feather=github></i></a>|<a class=soc href title=Twitter><i data-feather=twitter></i></a>|<a class=soc href title=Instagram><i data-feather=instagram></i></a>|<a class=soc href title=Bilibili><i data-feather=tv></i></a>|<a class=soc href=https://www.zhihu.com/people/wydnlz title=Zhihu><i data-feather=link></i></a>|⚡️
2021 © lunarwhite | <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a>
</footer>
<script>feather.replace()</script>
<script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script></div>
</body>
</html>