<!doctype html><html><head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge"><title>Deeplearning introduction - Ⅱ - lunarwhite</title><link rel=icon type=image/png href=favicon/favicon.ico><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Brief and customised manuals for me to check.">
<meta property="og:image" content>
<meta property="og:title" content="Deeplearning introduction - Ⅱ">
<meta property="og:description" content="Brief and customised manuals for me to check.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://lunarwhite.github.io/posts/guides/dl-intro2/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-06-24T20:21:48+00:00">
<meta property="article:modified_time" content="2021-06-24T20:21:48+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Deeplearning introduction - Ⅱ">
<meta name=twitter:description content="Brief and customised manuals for me to check.">
<script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,500&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel=stylesheet>
<link rel=stylesheet type=text/css media=screen href=https://lunarwhite.github.io/css/main.b787af43728695b428289b03901b2f1178d1db0f557c3284a5b068337d45655e.css>
<link disabled id=dark-mode-theme rel=stylesheet href=https://lunarwhite.github.io/css/dark.ce33575c8d9416a29a0c0f11d9c58c1ca865ac0b6e5ab9b37c0e3d89c5a5e4e1.css>
</head>
<body>
<div class=content><header>
<div class=main>
<a href=https://lunarwhite.github.io/>lunarwhite</a>
</div>
<nav>
<a href=/>Home</a>
<a href=/posts>Posts</a>
<a href=/tags>Tags</a>
<a href=/about>About</a>
/ <a id=dark-mode-toggle><i data-feather=sun></i></a>
</nav>
</header>
<main>
<article>
<div class=title>
<h1 class=title>Deeplearning introduction - Ⅱ</h1>
<div class=meta>Posted on Jun 24, 2021</div>
</div>
<section class=body>
<h2 id=1-目标检测>1 目标检测</h2>
<h3 id=11-简介>1.1 简介</h3>
<ul>
<li>目标定位</li>
<li>目标检测</li>
</ul>
<h3 id=12-过程>1.2 过程</h3>
<ul>
<li>全连接层转化成卷积层</li>
</ul>
<figure><img src=/images/dl-into2/01.png>
</figure>
<ul>
<li>卷积运算</li>
</ul>
<figure><img src=/images/dl-into2/02.png>
</figure>
<ul>
<li>用卷积实现滑动窗口检测
<ul>
<li>思路：首选确定一个矩形窗口——>然后将这个窗口按照一定的stride,遍历裁剪图片——>分别对每一张裁剪得到的图片，做图片分类</li>
<li>优点：共享权重参数，操作一次就可以完成，提高了整个算法的效率</li>
<li>缺点：边界框的位置可能不够准确</li>
</ul>
</li>
</ul>
<figure><img src=/images/dl-into2/03.png>
</figure>
<ul>
<li>定义Label
<ul>
<li>$[Pc,Bx,By,Bw,Bh,&mldr;..]$</li>
<li>$bx, by$：相对于单个小方格的位置，一定是$[0,1]$</li>
<li>$bh, bw$：相对于单个小方格边长的长度，可能$>1$</li>
</ul>
</li>
<li>Bounding-Box预测
<ul>
<li>将原图像划分为$n∗n$个小格</li>
<li>对于每一个对象，在标注的时候分析出其中点$b_x, b_y$，然后将这个对象的中点分配到对应的小格子中。因此一个对象最多被分配到一个小格子中</li>
<li>于每一个小格子，输出将是一个$5+K$的向量，$K$为类别数量</li>
</ul>
</li>
<li>IoU交并比（集合概念）
<ul>
<li>预测的边框，和真实的边框，的交集和并集的比值</li>
<li>衡量两个框重合的程度，=1完全重合，>=0.5可接受的</li>
</ul>
</li>
<li>非极大值抑制
<ul>
<li>防止一个物体被多次检验</li>
<li>首先去除$Pc$小于某个置信度的bounding box。然后 while true：找到当前的$Pc$最大的bounding box，去除所有与其IoU超过阈值的bounding box</li>
<li>如果有多种类物体待检验，则应该对每种类别单独进行一次非极大值抑制</li>
<li>如果一个bounding box内部有多个物品，则应该设置更小的bounding box</li>
</ul>
</li>
<li>Anchor-Boxes
<ul>
<li>让一个格检测出多个对象，需手动设置，或者kmeans自学习</li>
</ul>
</li>
</ul>
<h2 id=2-yolov1>2 YOLOv1</h2>
<h3 id=21-简介>2.1 简介</h3>
<figure><img src=/images/dl-into2/05.png>
</figure>
<ul>
<li>以前目标检测主要是利用分类器来进行检测</li>
<li>YOLO将目标检测中的空间边界框以及相关的类别概率作为一个回归问题来处理</li>
<li>单一的神经网络可以在一次评估中同时预测出边界框以及类别概率。一旦将整个检测流程作为一个神经网络来进行训练和预测， 其检测性能可以被直接端到端的优化</li>
</ul>
<h3 id=22-方法>2.2 方法</h3>
<ul>
<li>流程
<ul>
<li>将输入图像的大小调整为 448×448</li>
<li>在图像上运行单个卷积网络</li>
<li>通过模型的置信度对检测结果进行阈值化</li>
</ul>
</li>
<li>每个边界框
<ul>
<li>$x,y,w,h$以及置信度，共包含5个预测</li>
<li>$(x,y)$，表示相网格的中心坐标</li>
<li>$w,h$，表示预测的相对于整张图片的边界框的宽度和高度</li>
<li>置信度，表示预测边界框和真实边界框的$IOU$</li>
</ul>
</li>
</ul>
<h3 id=22-loss函数>2.2 Loss函数</h3>
<ul>
<li>包括
<ul>
<li>$coordError $，预测数据与标定数据之间的坐标误差</li>
<li>$iouError$，置信度能够等于预测边界框和真实边界框的交并比</li>
<li>$classError$，分类误差</li>
</ul>
</li>
</ul>
<figure><img src=/images/dl-into2/06.png>
</figure>
<figure><img src=/images/dl-into2/04.png>
</figure>
<figure><img src=/images/dl-into2/07.png>
</figure>
<h3 id=23-问题>2.3 问题</h3>
<ul>
<li>每个grid有30维，其中8维是回归box的坐标，2维是box的confidence，其余20维是类别。其中坐标的x,y用对应网格的offset归一化到0-1之间，w,h用图像的width和height归一化到0-1之间。</li>
<li>8维的localization error和20维的classification error同等重要显然是不合理的；如果一个网格中没有object（一幅图中这种网格很多），那么就会将这些网格中的box的confidence push到0，相比于较少的有object的网格，这种做法是overpowering的，这会导致网络不稳定甚至发散。</li>
</ul>
<h3 id=24-修正>2.4 修正</h3>
<ul>
<li>[1] 位置相关误差（坐标、$IOU$）与分类误差对网络$loss$的贡献值是不同的，因此YOLO在计算$loss$时，使用$λ_{coord}=5$修正$coordError$。</li>
<li>[2] 在计算$IOU$误差时，包含物体的格子与不包含物体的格子，二者的$IOU$误差对网络$loss$的贡献值是不同的。若采用相同的权值，那么不包含物体的格子的$confidence$值近似为$0$，变相放大了包含物体的格子的$confidence$误差在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用$λ_{noobj}=0.5$修正$iouError$，（注此处的包含是指存在一个物体，它的中心坐标落入到格子内）</li>
<li>[3] 对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为相同的位置偏差占大物体的比例远小于同等偏差占小物体的比例。YOLO将物体大小的信息项（$w$和$h$）进行求平方根来改进这个问题。（注：这个方法并不能完全解决这个问题）</li>
</ul>
<h3 id=25-优点>2.5 优点</h3>
<ul>
<li>具有开创意义，合并选框和检测</li>
<li>执行速度很快，因为它是一个端对端的网络(pipeline)</li>
<li>在预测中，对整张图的全局合理性把握得更好，因为YOLO在训练和测试中使用的是整张图片，这表明它在某种程度上编码了图片的类别和图片内容信息</li>
<li>能够提取到目标通用的特征，作者用自然图片训练，用艺术图片测试，效果好</li>
</ul>
<h3 id=26-缺点>2.6 缺点</h3>
<ul>
<li>在精确度上落后于当时最好的检测系统，主要来源于坐标定位的不准确</li>
<li>大的bounding box和小的bounding box对相同的小错误的敏感度相同</li>
<li>对密集物体，重叠面积大的物体不好预测</li>
<li>对相互靠的很近的物体，还有很小的群体检测效果不好</li>
</ul>
<h2 id=3-rnn>3 RNN</h2>
<h3 id=31-简介>3.1 简介</h3>
<ul>
<li>能够提取时间序列的信息，专门用来处理时间序列问题</li>
</ul>
<h3 id=32-原理>3.2 原理</h3>
<ul>
<li>$S_t=f(U∗X_t+W∗S_{t−1})$，$W=S_{t-1}$</li>
</ul>
<figure><img src=/images/dl-into2/08.png>
</figure>
<ul>
<li>网络中有个循环操作，能够保留之前学习到的内容，从而解决现在的问题</li>
<li>隐藏层的值$S$不仅仅取决于当前这次的输入$x$，还取决于上一次隐藏层的值$S$</li>
<li>权重矩阵$W$就是隐藏层上一次的值作为这一次的输入的权重</li>
</ul>
<h3 id=33-应用分类>3.3 应用分类</h3>
<ul>
<li>n VS n，输入和输出序列等长
<ul>
<li>计算视频中每一帧的分类标签</li>
</ul>
</li>
<li>n VS 1(Many to one)
<ul>
<li>情感分析，分类问题</li>
</ul>
</li>
<li>1 VS n
<ul>
<li>图像生成文字，给定体裁或风格生成音乐</li>
</ul>
</li>
<li>N vs M (Many to Many)，最重要变种
<ul>
<li>输入数据编码&mdash;>上下文向量C&mdash;> 另一个RNN网络，对其进行解码</li>
<li>机器翻译，文本摘要，阅读理解，语音识别</li>
<li>Encoder-Decoder模型， Seq2Seq模型</li>
</ul>
</li>
</ul>
<h3 id=34-lstm>3.4 LSTM</h3>
<ul>
<li>长短期记忆网络（Long Short-Term Memory networks）</li>
<li>与普通RNN区别
<ul>
<li>它们不再只是用一个单一的$tanh$层，而是用了四个相互作用的层</li>
</ul>
</li>
<li>门结构，实现添加或者删除信息的
<ul>
<li>三个门结构：forget gate ，input gate ，output gate</li>
<li>forget gate：决定让那些信息继续通过这个cell</li>
<li>input gate：决定让多少新的信息加入到cell状态中来</li>
</ul>
</li>
<li>解决梯度消失问题
<ul>
<li>RNN中一般会用$tanh()$函数作为激活函数，在迭代后期会逐渐收敛，导致梯度趋于0，出现了梯度下降</li>
<li>同或运算：第一部分与RNN的运算相同；第二部分通过三 个门增加</li>
</ul>
</li>
</ul>
<h3 id=34-情感分析实例>3.4 情感分析实例</h3>
<ul>
<li>1.原始文本：我喜欢文学</li>
<li>2.分词：我，喜欢，文学</li>
<li>3.索引化tokenize：[2, 345, 4564]，短句补全，长句剪裁</li>
<li>4.词向量化embedding：1-of-N encoding，Word Embedding</li>
<li>5.RNN网络</li>
<li>6.激活函数输出分类</li>
</ul>
<h2 id=4-自动编码器>4 自动编码器</h2>
<h3 id=41-简介>4.1 简介</h3>
<ul>
<li>是一种无监督的神经网络模型，可以学习到输入数据的隐含特征，这称为编码(coding)；同时用学习到的新特征可以重构出原始输入数据，称之为解码(decoding)</li>
<li>到了真正使用自编码的时候. 通常只会用到自编码前半部分</li>
<li>本质上是把样本的输入同时作为神经网络的输入和输出，通过最小化重构误差希望学习到样本的抽象特征表示$z$。这种无监督的优化方式大大提升了模型通用性</li>
</ul>
<h3 id=42-应用>4.2 应用</h3>
<ul>
<li>特征降维
<ul>
<li>从直观上来看，自动编码器可以用于特征降维，类似主成分分析PCA，但是其相比PCA其性能更强，这是由于神经网络模型可以提取更有效的新特征</li>
</ul>
</li>
<li>特征提取器
<ul>
<li>自动编码器学习到的新特征可以送入有监督学习模型中，起到特征提取器作用</li>
</ul>
</li>
<li>生成新数据
<ul>
<li>作为无监督学习模型，自动编码器还可以用于生成与训练样本不同的新数据，这样自动编码器（变分自动编码器，Variational Autoencoders）就是生成式模型，传统的AE不能作为生成模型）</li>
<li>变分自动编码器是一种特殊的自动编码器，它的训练过程加入了正则项，避免训练过程的过拟合，并且能确保其隐空间具有良好的能生成新样本的性质</li>
</ul>
</li>
</ul>
<h3 id=43-过程>4.3 过程</h3>
<ul>
<li>搭建一个AE需要完成
<ul>
<li>搭建编码器</li>
<li>搭建解码器</li>
<li>设定一个损失函数，用以衡量由于压缩而损失掉的信息</li>
</ul>
</li>
</ul>
<h2 id=5-gan>5 GAN</h2>
<h3 id=51-应用>5.1 应用</h3>
<ul>
<li>Image Generation</li>
<li>Sentence Generation</li>
</ul>
<h3 id=52-原理>5.2 原理</h3>
<ul>
<li>$G$
<ul>
<li>是一个生成网络，接收一个随机的噪声$z$，通过噪声生成图片，记做$G(z)$</li>
<li>目标：尽量生成真实的图片去欺骗判别网络$D$</li>
</ul>
</li>
<li>$D$
<ul>
<li>是一个判别网络，判别一张图片是不是真实的</li>
<li>输入参数是$x$，$x$代表一张图片，输出$D(x)$代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片</li>
<li>目标：尽量把$G$生成的图片和真实的图片分别开来</li>
</ul>
</li>
<li>博弈的结果
<ul>
<li>在最理想的状态下，$D(G(z))=0.5$</li>
<li>$G$可以生成足以以假乱真的图片$G(z)$</li>
<li>对于$D$来说，它难以判定$G$生成的图片究竟是不是真实的</li>
</ul>
</li>
</ul>
<figure><img src=/images/dl-into2/09.png>
</figure>
<h3 id=53-dcgan>5.3 DCGAN</h3>
<ul>
<li>原理
<ul>
<li>和GAN是一样的，它只是把上述的G和D换成了两个卷积神经网络。且对卷积神经网络的结构做了一些改变，以提高样本的质量和收敛的速度</li>
</ul>
</li>
<li>这些改变有
<ul>
<li>取消所有pooling层。G网络中使用转置卷积（transposed convolutional layer）进行上采样，D网络中用加入stride的卷积代替pooling</li>
<li>在D和G中均使用batch normalization</li>
<li>去掉FC层，使网络变为全卷积网络</li>
<li>G网络中使用ReLU作为激活函数，最后一层使用tanh</li>
<li>D网络中使用LeakyReLU作为激活函数</li>
</ul>
</li>
</ul>
</section>
<div class=post-tags>
<nav class="nav tags">
<ul class=tags>
<li><a href=/tags/deep-learning>deep-learning</a></li>
<li><a href=/tags/self-tutorial>self-tutorial</a></li>
</ul>
</nav>
</div>
</article>
</main>
<footer>
<hr><a class=soc href=https://www.linkedin.com/in/yuedongwoo/ title=LinkedIn><i data-feather=linkedin></i></a>|<a class=soc href=https://github.com/lunarwhite title=GitHub><i data-feather=github></i></a>|<a class=soc href title=Twitter><i data-feather=twitter></i></a>|<a class=soc href title=Instagram><i data-feather=instagram></i></a>|<a class=soc href title=Bilibili><i data-feather=tv></i></a>|<a class=soc href=https://www.zhihu.com/people/wydnlz title=Zhihu><i data-feather=link></i></a>|⚡️
2021 © lunarwhite | <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a>
</footer>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','G-80H8N7NN50','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<script src=https://lunarwhite.github.io/js/toggle.dark.js></script>
<script>feather.replace()</script>
<script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script></div>
</body>
</html>