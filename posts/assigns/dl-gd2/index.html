<!doctype html><html><head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge"><title>Figure out the best aurgments using gradient descent - Ⅱ - lunarwhite</title><link rel=icon type=image/png href=favicon/favicon.ico><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="summary for in-class assignments in SDU.">
<meta property="og:image" content>
<meta property="og:title" content="Figure out the best aurgments using gradient descent - Ⅱ">
<meta property="og:description" content="summary for in-class assignments in SDU.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://lunarwhite.github.io/posts/assigns/dl-gd2/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-04-17T11:38:49+00:00">
<meta property="article:modified_time" content="2021-04-17T11:38:49+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Figure out the best aurgments using gradient descent - Ⅱ">
<meta name=twitter:description content="summary for in-class assignments in SDU.">
<script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,500&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel=stylesheet>
<link rel=stylesheet type=text/css media=screen href=https://lunarwhite.github.io/css/main.f2d7282a0f5a7f5b27e43ccc9c3448ef92651f88fde45a8bac6c6e2a6ba2d01b.css>
<link disabled id=dark-mode-theme rel=stylesheet href=https://lunarwhite.github.io/css/dark.80b2eb1f37be2454ead72ba0f2662f3022d939734e00b0909a04541420aa9d6c.css>
</head>
<body>
<div class=content><header>
<div class=main>
<a href=https://lunarwhite.github.io>lunarwhite</a>
</div>
<nav>
<a href=/>Home</a>
<a href=/posts>Posts</a>
<a href=/tags>Tags</a>
<a href=/about>About</a>
/ <a id=dark-mode-toggle><i data-feather=sun></i></a>
</nav>
</header>
<main>
<article>
<div class=title>
<h1 class=title>Figure out the best aurgments using gradient descent - Ⅱ</h1>
<div class=meta>Posted on Apr 17, 2021</div>
</div>
<section class=body>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 采用模型y=b+wx，试着用梯度下降算法求出最优参数</span>
<span style=color:#75715e># 1.1 画出xdata，ydata的散点图</span>
<span style=color:#75715e># 1.2 画出线性回归的函数图</span>
<span style=color:#75715e># 2.1 画出b，w变化的图</span>
<span style=color:#75715e># 1.在作业三基础上实现Adagrad</span>
<span style=color:#75715e># 2.在作业三基础上，画出损失函数随迭代次数的变化的图</span>

<span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt

<span style=color:#75715e># 给定 xdata, ydata, 都为 10 维长的数组</span>
xdata <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>8.</span>, <span style=color:#ae81ff>3.</span>, <span style=color:#ae81ff>9.</span>, <span style=color:#ae81ff>7.</span>, <span style=color:#ae81ff>16.</span>, <span style=color:#ae81ff>05.</span>, <span style=color:#ae81ff>3.</span>, <span style=color:#ae81ff>10.</span>, <span style=color:#ae81ff>4.</span>, <span style=color:#ae81ff>6.</span>])
ydata <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>30.</span>, <span style=color:#ae81ff>21.</span>, <span style=color:#ae81ff>35.</span>, <span style=color:#ae81ff>27.</span>, <span style=color:#ae81ff>42.</span>, <span style=color:#ae81ff>24.</span>, <span style=color:#ae81ff>10.</span>, <span style=color:#ae81ff>38.</span>, <span style=color:#ae81ff>22.</span>, <span style=color:#ae81ff>25.</span>])

<span style=color:#75715e># init 超参数</span>
b <span style=color:#f92672>=</span> <span style=color:#ae81ff>70</span>  <span style=color:#75715e># 截距</span>
w <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>  <span style=color:#75715e># 斜率</span>
lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>  <span style=color:#75715e># 学习率</span>
iteration <span style=color:#f92672>=</span> <span style=color:#ae81ff>10000</span>  <span style=color:#75715e># 迭代次数</span>
eps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-10</span>

<span style=color:#75715e># functions</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cost</span>(b, w, xdata, ydata):
    sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, len(xdata)):
        sum <span style=color:#f92672>+=</span> (ydata[i] <span style=color:#f92672>-</span> (w <span style=color:#f92672>*</span> xdata[i] <span style=color:#f92672>+</span> b)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
    <span style=color:#66d9ef>return</span> sum <span style=color:#f92672>/</span> (len(xdata) <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>GD</span>(b, w, xdata, ydata, cache_b_w1):
    m <span style=color:#f92672>=</span> float(len(xdata))
    loss <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((iteration, <span style=color:#ae81ff>1</span>))
    epoch <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((iteration, <span style=color:#ae81ff>1</span>))

    <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> range(iteration):
        b_grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        w_grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>

        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(xdata)):
            b_grad <span style=color:#f92672>+=</span> (w <span style=color:#f92672>*</span> xdata[i] <span style=color:#f92672>+</span> b) <span style=color:#f92672>-</span> ydata[i]
            w_grad <span style=color:#f92672>+=</span> ((w <span style=color:#f92672>*</span> xdata[i] <span style=color:#f92672>+</span> b) <span style=color:#f92672>-</span> ydata[i]) <span style=color:#f92672>*</span> xdata[i]
        b_grad <span style=color:#f92672>=</span> b_grad <span style=color:#f92672>/</span> m
        w_grad <span style=color:#f92672>=</span> w_grad <span style=color:#f92672>/</span> m
        b <span style=color:#f92672>=</span> b <span style=color:#f92672>-</span> (lr <span style=color:#f92672>*</span> b_grad)
        w <span style=color:#f92672>=</span> w <span style=color:#f92672>-</span> (lr <span style=color:#f92672>*</span> w_grad)

        loss[item] <span style=color:#f92672>=</span> cost(b, w, xdata, ydata)
        epoch[item] <span style=color:#f92672>=</span> item
        cache_b_w1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack((cache_b_w1, [b, w]))

    <span style=color:#66d9ef>return</span> b, w, cache_b_w1, loss, epoch

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>AdaGrad</span>(b, w, xdata, ydata, lr, cache_b_w2):
    m <span style=color:#f92672>=</span> float(len(xdata))

    loss <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((iteration, <span style=color:#ae81ff>1</span>))
    epoch <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((iteration, <span style=color:#ae81ff>1</span>))

    <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> range(iteration):
        b_grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        w_grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>

        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(xdata)):
            b_grad <span style=color:#f92672>+=</span> (w <span style=color:#f92672>*</span> xdata[i] <span style=color:#f92672>+</span> b) <span style=color:#f92672>-</span> ydata[i]
            w_grad <span style=color:#f92672>+=</span> ((w <span style=color:#f92672>*</span> xdata[i] <span style=color:#f92672>+</span> b) <span style=color:#f92672>-</span> ydata[i]) <span style=color:#f92672>*</span> xdata[i]

        b_grad <span style=color:#f92672>=</span> b_grad <span style=color:#f92672>/</span> m
        w_grad <span style=color:#f92672>=</span> w_grad <span style=color:#f92672>/</span> m
        e <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>dot(xdata, np<span style=color:#f92672>.</span>dot(xdata, w) <span style=color:#f92672>-</span> ydata) <span style=color:#f92672>/</span> m
        tmp <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>square(e)
        w2 <span style=color:#f92672>=</span> lr <span style=color:#f92672>*</span> e <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sqrt(tmp <span style=color:#f92672>+</span> eps)

        b <span style=color:#f92672>=</span> b <span style=color:#f92672>-</span> (lr <span style=color:#f92672>*</span> b_grad)
        w <span style=color:#f92672>=</span> w <span style=color:#f92672>-</span> (lr <span style=color:#f92672>*</span> w_grad)
        loss[item] <span style=color:#f92672>=</span> cost(b, w, xdata, ydata)
        epoch[item] <span style=color:#f92672>=</span> item
        cache_b_w2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack((cache_b_w2, [b, w]))

    <span style=color:#66d9ef>return</span> b, w, cache_b_w2, loss, epoch

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>draw_contour</span>(cache_b_w1, cache_b_w2):
    row <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>200</span>)
    col <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>200</span>)

    B, W <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(row, col)
    Z <span style=color:#f92672>=</span> cost(B, W, xdata, ydata)

    plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>1</span>)
    plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;b&#39;</span>)
    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;w&#39;</span>)
    plt<span style=color:#f92672>.</span>contourf(B, W, Z, <span style=color:#ae81ff>100</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>)  <span style=color:#75715e># 填充轮廓</span>

    cache_b1 <span style=color:#f92672>=</span> cache_b_w1[:, <span style=color:#ae81ff>0</span>]
    cache_w1 <span style=color:#f92672>=</span> cache_b_w1[:, <span style=color:#ae81ff>1</span>]
    cache_b2 <span style=color:#f92672>=</span> cache_b_w2[:, <span style=color:#ae81ff>0</span>]
    cache_w2 <span style=color:#f92672>=</span> cache_b_w2[:, <span style=color:#ae81ff>1</span>]

    plt<span style=color:#f92672>.</span>plot(cache_b1, cache_w1)
    plt<span style=color:#f92672>.</span>plot(cache_b1, cache_w1, <span style=color:#e6db74>&#39;*&#39;</span>)
    plt<span style=color:#f92672>.</span>plot(cache_b2, cache_w2)
    plt<span style=color:#f92672>.</span>plot(cache_b2, cache_w2, <span style=color:#e6db74>&#39;-&#39;</span>)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>draw_scatter</span>():
    plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>1</span>)
    plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>)
    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;x&#39;</span>)
    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;y&#39;</span>)
    plt<span style=color:#f92672>.</span>plot(xdata, ydata, <span style=color:#e6db74>&#39;b.&#39;</span>)
    plt<span style=color:#f92672>.</span>plot(xdata, w <span style=color:#f92672>*</span> xdata <span style=color:#f92672>+</span> b, <span style=color:#e6db74>&#39;r&#39;</span>)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>draw_loss</span>(epoch1, loss1, epoch2, loss2):
    plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>1</span>)
    plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>)
    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;epoch&#39;</span>,)
    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss&#39;</span>)
    plt<span style=color:#f92672>.</span>plot(epoch1, loss1, <span style=color:#e6db74>&#39;kv-&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;GD&#39;</span>)
    plt<span style=color:#f92672>.</span>plot(epoch2, loss2, <span style=color:#e6db74>&#39;g.-&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;AdaGrad&#39;</span>)
    plt<span style=color:#f92672>.</span>legend()

<span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
    cache_b_w1 <span style=color:#f92672>=</span> [[b, w]]
    cache_b_w2 <span style=color:#f92672>=</span> [[b, w]]

    <span style=color:#75715e># gradient descent</span>
    b, w, cache_b_w1, loss1, epoch1 <span style=color:#f92672>=</span> GD(b, w, xdata, ydata, cache_b_w1)
    b, w, cache_b_w2, loss2, epoch2 <span style=color:#f92672>=</span> AdaGrad(b, w, xdata, ydata, lr, cache_b_w2)

    <span style=color:#75715e># draw plot</span>
    plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>6</span>))
    draw_contour(cache_b_w1, cache_b_w2)
    draw_scatter()
    draw_loss(epoch1, loss1, epoch2, loss2)
    plt<span style=color:#f92672>.</span>show()

</code></pre></div>
</section>
<div class=post-tags>
<nav class="nav tags">
<ul class=tags>
<li><a href=/tags/assginment>assginment</a></li>
<li><a href=/tags/deep-learning>deep-learning</a></li>
</ul>
</nav>
</div>
</article>
</main>
<footer>
<hr><a class=soc href=https://www.linkedin.com/in/yuedongwoo/ title=LinkedIn><i data-feather=linkedin></i></a>|<a class=soc href=https://github.com/lunarwhite title=GitHub><i data-feather=github></i></a>|<a class=soc href title=Twitter><i data-feather=twitter></i></a>|<a class=soc href title=Instagram><i data-feather=instagram></i></a>|<a class=soc href title=Bilibili><i data-feather=tv></i></a>|<a class=soc href=https://www.zhihu.com/people/wydnlz title=Zhihu><i data-feather=link></i></a>|⚡️
2021 © lunarwhite | <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a>
</footer>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','G-80H8N7NN50','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<script src=https://lunarwhite.github.iojs/toggle.dark.js></script>
<script>feather.replace()</script>
<script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script></div>
</body>
</html>