<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>high-performance-computing on lunarwhite</title><link>https://lunarwhite.github.io/tags/high-performance-computing/</link><description>Recent content in high-performance-computing on lunarwhite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© lunarwhite</copyright><lastBuildDate>Wed, 30 Jun 2021 10:38:49 +0000</lastBuildDate><atom:link href="https://lunarwhite.github.io/tags/high-performance-computing/index.xml" rel="self" type="application/rss+xml"/><item><title>HPC - MIC</title><link>https://lunarwhite.github.io/posts/guides/hpc-mic/</link><pubDate>Wed, 30 Jun 2021 10:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/guides/hpc-mic/</guid><description>1 初识与MIC 1.0 MIC应用模式 CPU原生模式 offload：CPU为主MIC为辅模式（最常用） CPU与MIC对等模式 MIC为主CPU为辅模式 native：MIC原生模式 1.1 offload, pi #include&amp;lt;stdio.h&amp;gt;#include&amp;lt;stdlib.h&amp;gt;#include&amp;lt;math.h&amp;gt; int main(){ float pi=0.0f; int count=10000; int i; #pragma offload target (mic) #pragma omp parallel for reduction(+:pi) for(i=0;i&amp;lt;count;i++){ float t=(float)((i+0.5f)/count); pi+=4.0f/(1.0f+t*t); } pi/=count; printf(&amp;#34;PI=%f\n&amp;#34;,pi); return 0; } icc -openmp 6-1.c -o 6-1.out -limf -lm ./6-1.out 1.2 offload, in #include&amp;lt;stdio.h&amp;gt;#include&amp;lt;stdlib.h&amp;gt;#include&amp;lt;math.h&amp;gt;#define LEN (10) int main(){ float* arr; int i; arr=(float*)malloc(LEN*sizeof(float)); for(i=0;i&amp;lt;LEN;++i){ arr[i]=i; } i=0; #pragma offload target(mic) in(arr:length(LEN)) for(i=0;i&amp;lt;LEN;++i){ printf(&amp;#34;on mic: arr[%d]=%f\n&amp;#34;,i,arr[i]); arr[i]=0; } for(i=0;i&amp;lt;LEN;++i){ printf(&amp;#34;on cpu: arr[%d]=%f\n&amp;#34;,i,arr[i]); } return 0; } icc 6-2.</description></item><item><title>HPC - CUDA</title><link>https://lunarwhite.github.io/posts/guides/hpc-cuda/</link><pubDate>Sun, 02 May 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/guides/hpc-cuda/</guid><description>1 初识与CUDA 1.0 基础概念 主机端-设备端（Host-Device） 内核函数（Kernel Function） 线程模型（Thread Model） 存储模型（Memory Model） 执行模型（CUDA Execution Model） GPU连接节点 ssh gpu01
1.1 kernel示例 __global__ void VecAdd(float* A, float* B, float* C){ int i = threadIdx.x; C[i] = A[i] + B[i]; } int main(){ VecAdd&amp;lt;&amp;lt;&amp;lt;1, N&amp;gt;&amp;gt;&amp;gt;(A, B, C); } 1.2 线程模型 __global__ void kernel(float* parameter); dim3 DimGrid(3, 2); //6 thread blocks dim3 DimBlock(16, 16); //256 threads per block kernel&amp;lt;&amp;lt;&amp;lt;DimGrid, DimBlock&amp;gt;&amp;gt;&amp;gt;(float* parameter) 1.</description></item><item><title>HPC - OpenMP</title><link>https://lunarwhite.github.io/posts/guides/hpc-openmp/</link><pubDate>Fri, 30 Apr 2021 19:18:01 +0000</pubDate><guid>https://lunarwhite.github.io/posts/guides/hpc-openmp/</guid><description>1 初识与OpenMP 1.1 OpenMP基本结构 #include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;omp.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#define NUM_THREADS 8 //设置线程数目为8 int main(){ //--串行区，由一个初始线程执行 static long num_steps=1000000000; double step; int i; double x,pi,sum=0.0; step=1.0/(double) num_steps; //调用运行时库函数，为后面的并行区设置线程数目 omp_set_num_threads(NUM_THREADS); //编译指导语句，创建一个并行区。在该并行区中多线程执行 #pragma omp parallel for reduction(+:sum) private(x) //--并行区，该区域由8个线程并行执行 for(i=0;i&amp;lt;num_steps;i++){ x=(i+0.5)*step; sum+=4.0/(1.0+x*x); } //--串行区，只有一个主线程执行 pi=step*sum; printf(&amp;#34;Pi=%21.20f(%ld steps)\n&amp;#34;,pi,num_steps); return 0; } icc -openmp -o 3-1 3-1.c ./3-1 1.2 循环并行化 #include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;omp.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#define NUM_THREADS 8 //设置线程数目为8 #define N 100 int main(){ //--串行区，由一个初始线程执行 int i; int x[N]; int y[N]; int z[N]; //初始化x[i] y[i] for(i=0; i&amp;lt;N; i++){ x[i]=i; y[i]=i*i; } //调用运行时库函数，为后面的并行区设置线程数目 omp_set_num_threads(NUM_THREADS); //编译指导语句，创建一个并行区。在该并行区中多线程执行 #pragma omp parallel for for(i=0; i&amp;lt;N; i++){ //--并行区，该区域由8个线程并行执行 z[i] = x[i]+y[i]; } //--串行区，只有一个主线程执行 for(i=0; i&amp;lt;N; i++){ printf(&amp;#34;z[%d]=x[%d]+y[%d]=%d\n&amp;#34;, i, i, i, z[i]); } return 0; } icc -openmp -o 3-2-2 3-2.</description></item><item><title>HPC - MPI</title><link>https://lunarwhite.github.io/posts/guides/hpc-mpi/</link><pubDate>Fri, 02 Apr 2021 18:18:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/guides/hpc-mpi/</guid><description>1 初识与MPI 1.1 hello-world #include&amp;#34;mpi.h&amp;#34;#include&amp;lt;stdio.h&amp;gt;#include&amp;lt;math.h&amp;gt;void main(int argc, char *argv[]){ // 相关变量声明 int numprocs; int procnum; int namelen; char pro_name[MPI_MAX_PROCESSOR_NAME]; // 初始化MPI环境 MPI_Init(&amp;amp;argc, &amp;amp;argv); // MPI程序体 MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;procnum); MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numprocs); MPI_Get_processor_name(pro_name, &amp;amp;namelen); printf(&amp;#34;Hello World! Hello HPC! Processor %d of %d on %s \n&amp;#34;, procnum, numprocs, pro_name); // 退出MPI环境 MPI_Finalize(); } // MPI并行程序的编译 mpicc -o 1-1 1-1.c // MPI程序运行，-np 或 -n 用来指定运行程序的进程数目 mpirun -np 4 1-1 1.2 简单的发送和接收 进程0向进程1发送一个整数，进程接收到后，将其打印到屏幕 #include&amp;lt;stdio.</description></item></channel></rss>