<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep-learning on lunarwhite</title><link>https://lunarwhite.github.io/tags/deep-learning/</link><description>Recent content in deep-learning on lunarwhite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© lunarwhite</copyright><lastBuildDate>Thu, 24 Jun 2021 20:21:48 +0000</lastBuildDate><atom:link href="https://lunarwhite.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Deeplearning introduction - Ⅱ</title><link>https://lunarwhite.github.io/posts/guides/dl-intro2/</link><pubDate>Thu, 24 Jun 2021 20:21:48 +0000</pubDate><guid>https://lunarwhite.github.io/posts/guides/dl-intro2/</guid><description>1 目标检测 1.1 简介 目标定位 目标检测 1.2 过程 全连接层转化成卷积层 卷积运算 用卷积实现滑动窗口检测 思路：首选确定一个矩形窗口——&amp;gt;然后将这个窗口按照一定的stride,遍历裁剪图片——&amp;gt;分别对每一张裁剪得到的图片，做图片分类 优点：共享权重参数，操作一次就可以完成，提高了整个算法的效率 缺点：边界框的位置可能不够准确 定义Label $[Pc,Bx,By,Bw,Bh,&amp;hellip;..]$ $bx, by$：相对于单个小方格的位置，一定是$[0,1]$ $bh, bw$：相对于单个小方格边长的长度，可能$&amp;gt;1$ Bounding-Box预测 将原图像划分为$n∗n$个小格 对于每一个对象，在标注的时候分析出其中点$b_x, b_y$，然后将这个对象的中点分配到对应的小格子中。因此一个对象最多被分配到一个小格子中 于每一个小格子，输出将是一个$5+K$的向量，$K$为类别数量 IoU交并比（集合概念） 预测的边框，和真实的边框，的交集和并集的比值 衡量两个框重合的程度，=1完全重合，&amp;gt;=0.5可接受的 非极大值抑制 防止一个物体被多次检验 首先去除$Pc$小于某个置信度的bounding box。然后 while true：找到当前的$Pc$最大的bounding box，去除所有与其IoU超过阈值的bounding box 如果有多种类物体待检验，则应该对每种类别单独进行一次非极大值抑制 如果一个bounding box内部有多个物品，则应该设置更小的bounding box Anchor-Boxes 让一个格检测出多个对象，需手动设置，或者kmeans自学习 2 YOLOv1 2.</description></item><item><title>Deeplearning introduction - Ⅰ</title><link>https://lunarwhite.github.io/posts/guides/dl-intro1/</link><pubDate>Thu, 24 Jun 2021 20:20:48 +0000</pubDate><guid>https://lunarwhite.github.io/posts/guides/dl-intro1/</guid><description>1 简介 1.1 一种定义 P（Performance）来评估计算机程序在某类任务T（Task）上的性能，若一个程序通过利用经验E（Experience）在T中任务上获得了性能改善，则我们就说关于T和P，该程序对E进行了学习 1.2 一种划分 分类Classification 输出值离散 e.g. 根据光谱的形状，分类为恒星、星系或者其它 回归Regression 输出连续值 e.g. 根据光谱的形状，估计恒星的温度 聚类Clustering 把相似的记录聚在一起，不需要标签 e.g. 对获得天体光谱聚类，相似的光谱被聚在同一组 1.3 三要素 数据、算法和模型 1.4 大致步骤 1 Define a Model 2 Goodness of Function 3 Pick the Best Function: Gradient decent 1.5 难点 调用算法得出了准确率？ 模型搭建过程以及评价指标算的是对的？ 模型选择，并且能说明模型选的合适？ 评价指标是不是符合真正的需求？ 训练集是否覆盖全面？ 数据里面有没有错误值、错误标签影响到了结果？ 获得的模型能真正用于实际应用？ 把过程进行清楚、有逻辑的表达，展示给别人？ 1.6 发展 深层网络训练中，梯度消失问题 方法：无监督预训练对权值进行初始化+有监督训练微调 ReLU激活函数被提出，能够有效的抑制梯度消失问题 Loss的局部极值问题 对于深层网络来说影响可以忽略 原因：深层网络虽然局部极值非常多，但是通过DL的BatchGradientDescent优化方法很难陷进去，而且就算陷进去，其局部极小值点与全局极小值点也是非常接近，但是浅层网络却不然，其拥有较少的局部极小值点，但是却很容易陷进去，且这些局部极小值点与全局极小值点相差较大 1.</description></item><item><title>Classify food images using CNN with Keras</title><link>https://lunarwhite.github.io/posts/assigns/dl-food-classify/</link><pubDate>Thu, 03 Jun 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-food-classify/</guid><description># to be continue: # 1. resize smaller # 2. 归一化 # 3. 期望acc 40-70% import os import cv2 import matplotlib.pyplot as plt import keras from keras_preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import * from keras.optimizers import * from keras.utils import plot_model workspace_dir = &amp;#39;../res/food-11&amp;#39; os.environ[&amp;#39;TF_CPP_MIN_LOG_LEVEL&amp;#39;] = &amp;#39;2&amp;#39; # init args image_dim = 128 model_loss = &amp;#39;categorical_crossentropy&amp;#39; model_optimizer = Adam() model_batch_size = 128 model_dropout = 0.2 model_epoch = 50 # 1 加载数据 def load_data(path, label): # label标记需不需要传y值：训练集和验证集需要y值-true，测试集不需要-false # listdir得到该路径下所有图片，sorted用于排序 image_dir = sorted(os.</description></item><item><title>Deeplearning frameworks</title><link>https://lunarwhite.github.io/posts/guides/dl-frameworks/</link><pubDate>Sat, 24 Apr 2021 20:20:48 +0000</pubDate><guid>https://lunarwhite.github.io/posts/guides/dl-frameworks/</guid><description>1 SkLearn 1.1 参考 https://sklearn.apachecn.org/ https://scikit-learn.org/stable/user_guide.html 2 Keras 2.1 参考 https://keras.io/zh/ 2.2 损失函数 二分类 binary_crossentropy 多分类 categorical_crossentropy one-hot编码 sparse_categorical_crossentropy 数字编码 kullback_leibler_divergence hinge 多用于SVM categorical_hinge squared_hinge poisson 回归 mean_squared_error mean_absolute_error mean_absolute_percentage_error mean_squared_logarithmic_error logcosh cosine_proximity 2.3 优化方法 列表 SGD() RMSprop() Adagrad() Adadelta() Adam() Adamax() Nadam()</description></item><item><title>Logistic regression and validity analysis</title><link>https://lunarwhite.github.io/posts/assigns/dl-lr-va/</link><pubDate>Sat, 24 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-lr-va/</guid><description># 请用逻辑回归预测xtest对应的y值（梯度下降自己实现）。 # 并计算输出 accuracy，precision，recall，auc import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import * xtrain = np.array([8., 3., 9., 7., 16., 05., 3., 10., 4., 6.]).reshape(-1, 1) ytrain = np.array([0, 0, 1, 0, 1, 0, 0, 1, 0, 0]).reshape(-1, 1) xtest = np.array([5., 4.5, 9.8, 8., 22., 17., 3., 19., 20, 30]).reshape(-1, 1) ytest = np.array([0, 0, 0, 1, 1, 1, 0, 1, 1, 1]).reshape(-1, 1) # init 超参数 b = 5 w = 1 lr = 0.</description></item><item><title>Figure out the best aurgments using gradient descent - Ⅱ</title><link>https://lunarwhite.github.io/posts/assigns/dl-gd2/</link><pubDate>Sat, 17 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-gd2/</guid><description># 采用模型y=b+wx，试着用梯度下降算法求出最优参数 # 1.1 画出xdata，ydata的散点图 # 1.2 画出线性回归的函数图 # 2.1 画出b，w变化的图 # 1.在作业三基础上实现Adagrad # 2.在作业三基础上，画出损失函数随迭代次数的变化的图 import numpy as np import matplotlib.pyplot as plt # 给定 xdata, ydata, 都为 10 维长的数组 xdata = np.array([8., 3., 9., 7., 16., 05., 3., 10., 4., 6.]) ydata = np.array([30., 21., 35., 27., 42., 24., 10., 38., 22., 25.]) # init 超参数 b = 70 # 截距 w = 8 # 斜率 lr = 0.01 # 学习率 iteration = 10000 # 迭代次数 eps = 1e-10 # functions def cost(b, w, xdata, ydata): sum = 0 for i in range(0, len(xdata)): sum += (ydata[i] - (w * xdata[i] + b)) ** 2 return sum / (len(xdata) * 2) def GD(b, w, xdata, ydata, cache_b_w1): m = float(len(xdata)) loss = np.</description></item><item><title>Figure out the best aurgments using gradient descent - Ⅰ</title><link>https://lunarwhite.github.io/posts/assigns/dl-gd1/</link><pubDate>Sun, 11 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-gd1/</guid><description># 采用模型y=b+wx，试着用梯度下降算法求出最优参数 # 1.1 画出xdata，ydata的散点图 # 1.2 画出线性回归的函数图 # 2.1 画出b，w变化的图 import numpy as np import matplotlib.pyplot as plt # 给定 xdata, ydata, 都为 10 维长的数组 xdata = np.array([8., 3., 9., 7., 16., 05., 3., 10., 4., 6.]) ydata = np.array([30., 21., 35., 27., 42., 24., 10., 38., 22., 25.]) # init 超参数 b = 70 # 截距 w = 8 # 斜率 lr = 0.01 # 学习率 iteration = 1000 # 迭代次数 # functions def cost(b, w, xdata, ydata): sum = 0 for i in range(0, len(xdata)): sum += (ydata[i] - (w * xdata[i] + b)) ** 2 return sum / (len(xdata) * 2) def gradient_descent(b, w, xdata, ydata, lr, iteration, cache_b_w): m = float(len(xdata)) for item in range(iteration): b_grad = 0 w_grad = 0 for i in range(len(xdata)): b_grad += (w * xdata[i] + b) - ydata[i] w_grad += ((w * xdata[i] + b) - ydata[i]) * xdata[i] b_grad = b_grad / m w_grad = w_grad / m b = b - (lr * b_grad) w = w - (lr * w_grad) cache_b_w = np.</description></item><item><title>Fit cicada frequency and temperature using linear regression</title><link>https://lunarwhite.github.io/posts/assigns/dl-lr-cicada/</link><pubDate>Sat, 03 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-lr-cicada/</guid><description>import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.metrics import mean_squared_error # dataset xtrain = [8., 3., 9., 7., 16., 5., 13., 10., 4., 6.] ytrain = [30., 21., 33., 27., 42., 24., 36., 33., 22., 25.] xtest = [4.5, 6, 14] ytest = [24, 25, 38] xtrain = np.array(xtrain).reshape(-1, 1) ytrain = np.array(ytrain).reshape(-1, 1) xtest = np.array(xtest).reshape(-1, 1) ytest = np.array(ytest).reshape(-1, 1) # 利用sklearn包中的linear_model.LinearRegression # （1）预测蝉每小时鸣叫次数和温度之间的函数关系 clf = linear_model.</description></item></channel></rss>