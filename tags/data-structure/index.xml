<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>data-structure on lunarwhite</title><link>https://lunarwhite.github.io/tags/data-structure/</link><description>Recent content in data-structure on lunarwhite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© lunarwhite</copyright><lastBuildDate>Mon, 21 Jun 2021 11:12:00 +0000</lastBuildDate><atom:link href="https://lunarwhite.github.io/tags/data-structure/index.xml" rel="self" type="application/rss+xml"/><item><title>COVID19 sentiment data analysis</title><link>https://lunarwhite.github.io/posts/assigns/ds-covid/</link><pubDate>Mon, 21 Jun 2021 11:12:00 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/ds-covid/</guid><description>本项目为数据结构课程设计的设计&amp;amp;开发文档
代码仓库在这里
1 引言 1.1 编写目的 公众情绪一直复杂多元，随着信息化程度的提高与大数据、人工智能等技术的不断普及又得以放大，让更多人的情感和想法得以在网络上得到表达与传播，传播范围比以往更大。新冠疫情于去年一月开始爆发，转眼间已经过去了一年多，疫情在国内得到了有效的控制。回顾这个过程，疫情下形成了特殊的网络社会心态和公众情绪。多元复杂的公众情绪，借助网络的力量传播和放大。但也方便了收集数据，并研究情绪变化的具体过程。
因此基于此次疫情，借助适宜的数据、分析手段和自然语言处理技术，希望在一定程度上了解新冠疫情这一特殊事件，在其自身不同发展阶段对中国大众心态的影响，以大数据技术研究中国大众的网络社会心态及其变化规律，进而形成引导公众情绪、维护社会稳定的参考性依据，或许有助于未来的类似事件的应对。
1.2 项目概述 以微博为代表的社交媒体上广泛的传播各种疫情信息，在疫情阶段发挥着比较重要的信息传输作用。本次作业的目的就是深入分析疫情信息中蕴含的网民情绪及其变化情况。以新冠肺炎疫情相关的短微博和相关新闻下的评论作为主要研究对象，首先爬取大量数据，利用心态词典方法可以大致观察心态演变，并结合层次聚类法从中分析网民关注热点，最后通过可视化方法展现相应的结果。
1.3 可行性分析 1）市场可行性：有助于平台运营人员乃至普通群众以可视化这种友好的交互方式分析、及时准确把握舆情变化，和不同阶段大众心态的影响。进而便于引导公众情绪、维护社会稳定，并对未来的类似突发事件的应对产生参考性意义。
2）技术可行性：数据源、数据量的支持，爬虫技术的成熟，机器学习尤其是自然语言处理方向发展如火如荼，文本情感分析技术趋于成熟，Python 有大量的可视化分析的第三方库，如pyecharts、matplotlib等。
1.4 术语和缩略语 [1] 聚类分析：聚类分析（英语：Cluster analysis）亦称为群集分析，是对于统计数据分析的一门技术，在许多领域受到广泛应用，包括机器学习，数据挖掘，模式识别，图像分析以及生物信息。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。一般把数据聚类归纳为一种非监督式学习。
[2] 文本情感分析：文本情感分析（也称为意见挖掘）是指用自然语言处理、文本挖掘以及计算机语言学等方法来识别和提取原素材中的主观信息。通常来说，情感分析的目的是为了找出说话者/作者在某些话题上或者针对一个文本两极的观点的态度。这个态度或许是他或她的个人判断或是评估，也许是他当时的情感状态（就是说，作者在做出这个言论时的情绪状态），或是作者有意向的情感交流（就是作者想要读者所体验的情绪）。
[3] 文本分割：文本分割（Text segmentation）将书面文本分割成有意义单位的过程，如单词、句子或主题。这个术语既适用于人类阅读文本时的心理过程，也适用于在计算机中实现的人工过程，后者属于自然语言处理的领域。一些书面语言有明确的单词分界标记，例如英语的词之间有空格标识，阿拉伯语有独特的首、中、末字母形状，但这种标记不是所有书面语言都有。
[4] 停用词：在信息检索中，为节省存储空间和提高搜索效率，在自然语言处理数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words(停用词)。不要把停用词与安全口令混淆。 这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表。但是，并没有一个明确的停用词表能够适用于所有的工具。甚至有一些工具是明确地避免使用停用词来支持短语搜索的。
[5] 网络爬虫：网络爬虫（英语：web crawler），也叫网络蜘蛛（spider），是一种用来自动浏览万维网的网络机器人。其目的一般为编纂网络索引。 网络搜索引擎等站点通过爬虫软件更新自身的网站内容或其对其他网站的索引。网络爬虫可以将自己所访问的页面保存下来，以便搜索引擎事后生成索引供用户搜索。 爬虫访问网站的过程会消耗目标系统资源。不少网络系统并不默许爬虫工作。因此在访问大量页面时，爬虫需要考虑到规划、负载，还需要讲“礼貌”。 不愿意被爬虫访问、被爬虫主人知晓的公开站点可以使用 robots.txt 文件之类的方法避免访问。这个文件可以要求机器人只对网站的一部分进行索引，或完全不作处理。
1.5 参考资料 [1] Scrapy Tutorial https://docs.scrapy.org/en/latest/intro/tutorial.html
[2] Matplotlib Tutorial https://matplotlib.org/stable/tutorials/index.html
[3] Documenation of scikit-learn 0.19.1 https://sklearn.org/documentation.html
[4] scikit-learn (sklearn) 官方文档中文版 https://sklearn.apachecn.org
[5] HanLP: Han Language Processing https://hanlp.hankcs.com/docs/
[6] Natural Language Toolkit 3.</description></item><item><title>Huffman encoding and decoding using Python and PyQt</title><link>https://lunarwhite.github.io/posts/assigns/ds-gui-huffman/</link><pubDate>Fri, 26 Mar 2021 20:50:38 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/ds-gui-huffman/</guid><description>1 要求 1.1 简介 初始化 (Initialization)：从终端读入n个字符，建立哈夫曼树； 编码 (Coding)：利用已建好的哈夫曼树，对字符进行编码，然后将正文编码结果存入文件codefile中； 译码 (Decoding)：利用已建好的哈夫曼树将文件codefile中的代码进行译码，结果存入文件textfile中。 1.2 思路 （待写 2 实现 2.1 环境 Python3.8.5 PyQt5.15.4 PyCharm2020.3.3 2.2 代码 import os import sys from PyQt5 import QtCore, QtWidgets, QtGui from PyQt5.QtWidgets import QMessageBox path = os.getcwd() encoder = {} decoder = {} class Ui_MainWindow(object): def setupUi(self, MainWindow): MainWindow.setObjectName(&amp;#34;MainWindow&amp;#34;) MainWindow.resize(495, 328) self.centralwidget = QtWidgets.QWidget(MainWindow) self.centralwidget.setObjectName(&amp;#34;centralwidget&amp;#34;) self.textEdit = QtWidgets.QTextEdit(self.centralwidget) self.textEdit.setGeometry(QtCore.QRect(10, 10, 341, 291)) self.</description></item><item><title>File word retrieval and statistics using Python</title><link>https://lunarwhite.github.io/posts/assigns/ds-word-stat/</link><pubDate>Sat, 19 Dec 2020 19:48:51 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/ds-word-stat/</guid><description>1 要求 1.1 简介 给定一个文本文件，要求统计给定单词在文本中出现的总次数，并检索输出某个单词出现在文本中的行号、在该行中出现的次数以及位置。
1.2思路 可以分为主控菜单程序和另外三个部分实现：
零、主控菜单程序 (1) 头文件包含 (2) 菜单选项包括 建立文件 单词计数 单词定位 退出程序 (3) 选择1-4执行相应的操作，其他字符为非法 一、建立一个文本文件，文件名由用户用键盘输入 实现过程 (1) 定义一个串变量 (2) 定义文本文件 (3) 输入文件名，打开该文件 (4) 循环读入文本行，写入文本文件 (5) 关闭文件 二、给定单词计数，输入一个不含空格的单词，统计输出该单词在文本中的出现次数 实现设计 要用到模式匹配算法，逐行扫描文本文件。匹配一个，计数器加1，直到整个文件扫描结束；然后输出单词出现的次数 朴素模式匹配算法的基本思路是将给定字串与主串从第一个字符开始比较，找到首次与子串完全匹配的子串为止，并记住该位置。 但为了实现统计子串出现的个数，不仅需要从主串的第一个字符位置开始比较，而且需要从主串的任一位置检索匹配字符串。 实现过程 (1) 输入要检索的文本文件名，打开相应的文件 (2) 输入要检索统计的单词 (3) 循环读文本文件，读入一行，将其送入定义好的串中，并求该串的实际长度，调用串匹配函数进行计数 (4) 关闭文件，输出统计结果 三、检索给定单词，输入一个单词，检索并输出该单词所在的行号、该行中出现的次数以及在该行中的相应位置。 实现设计 同上一个设计类似，但是要相对复杂一些 实现过程 (1) 输入要检索的文本文件名，打开相应的文件 (2) 输入要检索统计的单词 (3) 循环读文本文件，读入一行，将其送入定义好的串中，并求该串的实际长度。行计数器置初值0，调用串匹配函数进行计数。如果 行单词计数器!</description></item></channel></rss>