<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>assginment on lunarwhite</title><link>https://lunarwhite.github.io/tags/assginment/</link><description>Recent content in assginment on lunarwhite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© lunarwhite</copyright><lastBuildDate>Mon, 21 Jun 2021 11:12:00 +0000</lastBuildDate><atom:link href="https://lunarwhite.github.io/tags/assginment/index.xml" rel="self" type="application/rss+xml"/><item><title>COVID19 sentiment data analysis</title><link>https://lunarwhite.github.io/posts/assigns/ds-covid/</link><pubDate>Mon, 21 Jun 2021 11:12:00 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/ds-covid/</guid><description>本项目为数据结构课程设计的设计&amp;amp;开发文档
代码仓库在这里
1 引言 1.1 编写目的 公众情绪一直复杂多元，随着信息化程度的提高与大数据、人工智能等技术的不断普及又得以放大，让更多人的情感和想法得以在网络上得到表达与传播，传播范围比以往更大。新冠疫情于去年一月开始爆发，转眼间已经过去了一年多，疫情在国内得到了有效的控制。回顾这个过程，疫情下形成了特殊的网络社会心态和公众情绪。多元复杂的公众情绪，借助网络的力量传播和放大。但也方便了收集数据，并研究情绪变化的具体过程。
因此基于此次疫情，借助适宜的数据、分析手段和自然语言处理技术，希望在一定程度上了解新冠疫情这一特殊事件，在其自身不同发展阶段对中国大众心态的影响，以大数据技术研究中国大众的网络社会心态及其变化规律，进而形成引导公众情绪、维护社会稳定的参考性依据，或许有助于未来的类似事件的应对。
1.2 项目概述 以微博为代表的社交媒体上广泛的传播各种疫情信息，在疫情阶段发挥着比较重要的信息传输作用。本次作业的目的就是深入分析疫情信息中蕴含的网民情绪及其变化情况。以新冠肺炎疫情相关的短微博和相关新闻下的评论作为主要研究对象，首先爬取大量数据，利用心态词典方法可以大致观察心态演变，并结合层次聚类法从中分析网民关注热点，最后通过可视化方法展现相应的结果。
1.3 可行性分析 1）市场可行性：有助于平台运营人员乃至普通群众以可视化这种友好的交互方式分析、及时准确把握舆情变化，和不同阶段大众心态的影响。进而便于引导公众情绪、维护社会稳定，并对未来的类似突发事件的应对产生参考性意义。
2）技术可行性：数据源、数据量的支持，爬虫技术的成熟，机器学习尤其是自然语言处理方向发展如火如荼，文本情感分析技术趋于成熟，Python 有大量的可视化分析的第三方库，如pyecharts、matplotlib等。
1.4 术语和缩略语 [1] 聚类分析：聚类分析（英语：Cluster analysis）亦称为群集分析，是对于统计数据分析的一门技术，在许多领域受到广泛应用，包括机器学习，数据挖掘，模式识别，图像分析以及生物信息。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。一般把数据聚类归纳为一种非监督式学习。
[2] 文本情感分析：文本情感分析（也称为意见挖掘）是指用自然语言处理、文本挖掘以及计算机语言学等方法来识别和提取原素材中的主观信息。通常来说，情感分析的目的是为了找出说话者/作者在某些话题上或者针对一个文本两极的观点的态度。这个态度或许是他或她的个人判断或是评估，也许是他当时的情感状态（就是说，作者在做出这个言论时的情绪状态），或是作者有意向的情感交流（就是作者想要读者所体验的情绪）。
[3] 文本分割：文本分割（Text segmentation）将书面文本分割成有意义单位的过程，如单词、句子或主题。这个术语既适用于人类阅读文本时的心理过程，也适用于在计算机中实现的人工过程，后者属于自然语言处理的领域。一些书面语言有明确的单词分界标记，例如英语的词之间有空格标识，阿拉伯语有独特的首、中、末字母形状，但这种标记不是所有书面语言都有。
[4] 停用词：在信息检索中，为节省存储空间和提高搜索效率，在自然语言处理数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words(停用词)。不要把停用词与安全口令混淆。 这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表。但是，并没有一个明确的停用词表能够适用于所有的工具。甚至有一些工具是明确地避免使用停用词来支持短语搜索的。
[5] 网络爬虫：网络爬虫（英语：web crawler），也叫网络蜘蛛（spider），是一种用来自动浏览万维网的网络机器人。其目的一般为编纂网络索引。 网络搜索引擎等站点通过爬虫软件更新自身的网站内容或其对其他网站的索引。网络爬虫可以将自己所访问的页面保存下来，以便搜索引擎事后生成索引供用户搜索。 爬虫访问网站的过程会消耗目标系统资源。不少网络系统并不默许爬虫工作。因此在访问大量页面时，爬虫需要考虑到规划、负载，还需要讲“礼貌”。 不愿意被爬虫访问、被爬虫主人知晓的公开站点可以使用 robots.txt 文件之类的方法避免访问。这个文件可以要求机器人只对网站的一部分进行索引，或完全不作处理。
1.5 参考资料 [1] Scrapy Tutorial https://docs.scrapy.org/en/latest/intro/tutorial.html
[2] Matplotlib Tutorial https://matplotlib.org/stable/tutorials/index.html
[3] Documenation of scikit-learn 0.19.1 https://sklearn.org/documentation.html
[4] scikit-learn (sklearn) 官方文档中文版 https://sklearn.apachecn.org
[5] HanLP: Han Language Processing https://hanlp.hankcs.com/docs/
[6] Natural Language Toolkit 3.</description></item><item><title>Classify food images using CNN with Keras</title><link>https://lunarwhite.github.io/posts/assigns/dl-food-classify/</link><pubDate>Thu, 03 Jun 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-food-classify/</guid><description># to be continue: # 1. resize smaller # 2. 归一化 # 3. 期望acc 40-70% import os import cv2 import matplotlib.pyplot as plt import keras from keras_preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import * from keras.optimizers import * from keras.utils import plot_model workspace_dir = &amp;#39;../res/food-11&amp;#39; os.environ[&amp;#39;TF_CPP_MIN_LOG_LEVEL&amp;#39;] = &amp;#39;2&amp;#39; # init args image_dim = 128 model_loss = &amp;#39;categorical_crossentropy&amp;#39; model_optimizer = Adam() model_batch_size = 128 model_dropout = 0.2 model_epoch = 50 # 1 加载数据 def load_data(path, label): # label标记需不需要传y值：训练集和验证集需要y值-true，测试集不需要-false # listdir得到该路径下所有图片，sorted用于排序 image_dir = sorted(os.</description></item><item><title>Tiny zhihu-like web - deploy doc</title><link>https://lunarwhite.github.io/posts/assigns/db-tiny-zh2/</link><pubDate>Tue, 01 Jun 2021 11:12:00 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/db-tiny-zh2/</guid><description>本项目为数据库课程设计的开发文档
设计文档在这里，代码仓库在这里
欢迎交流拍砖~
1 云服务器 1.1 连接 服务器信息 操作系统：Ubuntu Server 18.04 LTS 配置：标准型S5/1核/2GB/1Mbps 系统盘：高效云硬盘/50GB 1.2 域名 备案 ID证、地址、手机号、Email 解析 管理： 解析控制台 二级域名：blog / demo / &amp;hellip; 端口 管理：配置安全组 SSL 支持二级、三级域名，同一主域最多20张免费证书 有效期一年，到期后需要重新申请并安装 TrustAsia TLS RSA CA证书（年限：1年） 管理：证书管理控制台 域名验证方式：DNS 验证 SSL部署 官方文档：Nginx 服务器 SSL 证书安装部署 下载证书，解压之后选择Nginx文件夹，重命名为cert .crt文件：是证书文件，crt是pem文件的扩展名 .key文件：证书的私钥文件 在Nginx的安装目录下创建cert目录，将本地证书文件和密钥文件上传 sudo mkdir /etc/nginx/cert sudo chmod 777 /etc/nginx/cert sudo scp -r 1_lunarwhite.</description></item><item><title>Tiny zhihu-like web - design doc</title><link>https://lunarwhite.github.io/posts/assigns/db-tiny-zh1/</link><pubDate>Mon, 03 May 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/db-tiny-zh1/</guid><description>本项目为数据库课程设计的设计文档
开发文档在这里，代码仓库在这里
欢迎交流拍砖~
1 选题说明及需求介绍 1.1 选题说明 主要针对数据库编程的设计和实现。选择设计实现一个仿 「知乎」问答社区系统。今年三月，成立十周年的知乎在美国纽约交易所隆重上市。 知乎十年以来，知乎使用过的标语有：“认真你就赢了”、“我们 都是有问题的人”、“有问题，上知乎”和“有问题，就会有答案”等。可见，问题与答案是知乎的灵魂、是知乎的运营标准和产品定位。 1.2 需求介绍 知乎是知名的中文问答社区，连接各行各业的用户。人们普遍认为它是中文版的 Quora，高质量中文知识社区。用户分享着彼此的知识、经验和见 解，为中文互联网源源不断地提供多种多样的信息。准确地讲，知乎更像一个论坛：用户围绕着某一感兴趣的话题进行相关的讨论，同时可以关注兴趣一致 的人。对于概念性的解释，网络百科几乎涵盖了你所有的疑问；但是对于发散思维的整合，却是知乎的一大特色。 用户通过搜索、浏览其他用户的问题的回答，解决自己的一些问题，同时可以发布问题与回答，进行友好交流；同时，可以点赞你认可的回答与提问、关注用户，实现简易的社交功能。对于管理员可以进行内容审核，对论坛中一些不合法的信息进行删除，甚至可以封禁用户；对于积极输出某一专业知识领域的优质内容的友善用户，授予「优秀回答者」荣誉称号，鼓励用户创作。 2 系统的功能模块划分 2.1 需求分析 在该社区中已登录用户的操作： ①浏览，用户可以根据个人兴趣进行浏览，包括问题以及问题下面的回 答、回答下面的评论。 ②提问，用户可以对问题进行简单描述，发布，等待其他用户回答问 题。发布之后可以编辑、删除。 ③回答，用户可以针对问题发表自己的见解，分享经验。 ④点赞，用户可以对喜欢的回答进行点赞、推荐，让更多的人看到。 ⑤关注，用户可以对自己感兴趣的人进行关注、对感兴趣的问题关注。 ⑥收藏，用户可以创建和删除收藏夹，收藏回答，方便下次阅读。 ⑦个人主页，用户拥有自己主页和个人资料，可以查看自己发布和收藏。 ⑧删除，用户只有权对自己发布的问题、回答进行删除。 ⑨搜索，用户通过输入关键字进行搜索，返回相关问题、回答以及用户。 在该社区中管理员的操作： ①对发表的不合法的提问、回答和评论进行删除。 ②对发表不合法信息的用户进行禁言甚至删除。 ③内容审核，编辑问题或删除问题、回答。 ④对于积极输出优质内容的用户授予「优秀回答者」荣誉称号。 未注册的临时用户的操作： ①浏览，用户可以根据个人兴趣进行浏览，包括问题以及问题下面的回 答、回答下面的评论。 ②搜索，用户通过输入关键字进行搜索，返回相关问题、回答以及用 户。 2.2 模块划分 整体功能模块图 2.3 数据流图 顶层数据流图 一层数据流图-临时用户 一层数据流图-注册用户 一层数据流图-平台管理员 2.</description></item><item><title>Logistic regression and validity analysis</title><link>https://lunarwhite.github.io/posts/assigns/dl-lr-va/</link><pubDate>Sat, 24 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-lr-va/</guid><description># 请用逻辑回归预测xtest对应的y值（梯度下降自己实现）。 # 并计算输出 accuracy，precision，recall，auc import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import * xtrain = np.array([8., 3., 9., 7., 16., 05., 3., 10., 4., 6.]).reshape(-1, 1) ytrain = np.array([0, 0, 1, 0, 1, 0, 0, 1, 0, 0]).reshape(-1, 1) xtest = np.array([5., 4.5, 9.8, 8., 22., 17., 3., 19., 20, 30]).reshape(-1, 1) ytest = np.array([0, 0, 0, 1, 1, 1, 0, 1, 1, 1]).reshape(-1, 1) # init 超参数 b = 5 w = 1 lr = 0.</description></item><item><title>Figure out the best aurgments using gradient descent - Ⅱ</title><link>https://lunarwhite.github.io/posts/assigns/dl-gd2/</link><pubDate>Sat, 17 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-gd2/</guid><description># 采用模型y=b+wx，试着用梯度下降算法求出最优参数 # 1.1 画出xdata，ydata的散点图 # 1.2 画出线性回归的函数图 # 2.1 画出b，w变化的图 # 1.在作业三基础上实现Adagrad # 2.在作业三基础上，画出损失函数随迭代次数的变化的图 import numpy as np import matplotlib.pyplot as plt # 给定 xdata, ydata, 都为 10 维长的数组 xdata = np.array([8., 3., 9., 7., 16., 05., 3., 10., 4., 6.]) ydata = np.array([30., 21., 35., 27., 42., 24., 10., 38., 22., 25.]) # init 超参数 b = 70 # 截距 w = 8 # 斜率 lr = 0.01 # 学习率 iteration = 10000 # 迭代次数 eps = 1e-10 # functions def cost(b, w, xdata, ydata): sum = 0 for i in range(0, len(xdata)): sum += (ydata[i] - (w * xdata[i] + b)) ** 2 return sum / (len(xdata) * 2) def GD(b, w, xdata, ydata, cache_b_w1): m = float(len(xdata)) loss = np.</description></item><item><title>Figure out the best aurgments using gradient descent - Ⅰ</title><link>https://lunarwhite.github.io/posts/assigns/dl-gd1/</link><pubDate>Sun, 11 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-gd1/</guid><description># 采用模型y=b+wx，试着用梯度下降算法求出最优参数 # 1.1 画出xdata，ydata的散点图 # 1.2 画出线性回归的函数图 # 2.1 画出b，w变化的图 import numpy as np import matplotlib.pyplot as plt # 给定 xdata, ydata, 都为 10 维长的数组 xdata = np.array([8., 3., 9., 7., 16., 05., 3., 10., 4., 6.]) ydata = np.array([30., 21., 35., 27., 42., 24., 10., 38., 22., 25.]) # init 超参数 b = 70 # 截距 w = 8 # 斜率 lr = 0.01 # 学习率 iteration = 1000 # 迭代次数 # functions def cost(b, w, xdata, ydata): sum = 0 for i in range(0, len(xdata)): sum += (ydata[i] - (w * xdata[i] + b)) ** 2 return sum / (len(xdata) * 2) def gradient_descent(b, w, xdata, ydata, lr, iteration, cache_b_w): m = float(len(xdata)) for item in range(iteration): b_grad = 0 w_grad = 0 for i in range(len(xdata)): b_grad += (w * xdata[i] + b) - ydata[i] w_grad += ((w * xdata[i] + b) - ydata[i]) * xdata[i] b_grad = b_grad / m w_grad = w_grad / m b = b - (lr * b_grad) w = w - (lr * w_grad) cache_b_w = np.</description></item><item><title>Fit cicada frequency and temperature using linear regression</title><link>https://lunarwhite.github.io/posts/assigns/dl-lr-cicada/</link><pubDate>Sat, 03 Apr 2021 11:38:49 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/dl-lr-cicada/</guid><description>import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.metrics import mean_squared_error # dataset xtrain = [8., 3., 9., 7., 16., 5., 13., 10., 4., 6.] ytrain = [30., 21., 33., 27., 42., 24., 36., 33., 22., 25.] xtest = [4.5, 6, 14] ytest = [24, 25, 38] xtrain = np.array(xtrain).reshape(-1, 1) ytrain = np.array(ytrain).reshape(-1, 1) xtest = np.array(xtest).reshape(-1, 1) ytest = np.array(ytest).reshape(-1, 1) # 利用sklearn包中的linear_model.LinearRegression # （1）预测蝉每小时鸣叫次数和温度之间的函数关系 clf = linear_model.</description></item><item><title>Huffman encoding and decoding using Python and PyQt</title><link>https://lunarwhite.github.io/posts/assigns/ds-gui-huffman/</link><pubDate>Fri, 26 Mar 2021 20:50:38 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/ds-gui-huffman/</guid><description>1 要求 1.1 简介 初始化 (Initialization)：从终端读入n个字符，建立哈夫曼树； 编码 (Coding)：利用已建好的哈夫曼树，对字符进行编码，然后将正文编码结果存入文件codefile中； 译码 (Decoding)：利用已建好的哈夫曼树将文件codefile中的代码进行译码，结果存入文件textfile中。 1.2 思路 （待写 2 实现 2.1 环境 Python3.8.5 PyQt5.15.4 PyCharm2020.3.3 2.2 代码 import os import sys from PyQt5 import QtCore, QtWidgets, QtGui from PyQt5.QtWidgets import QMessageBox path = os.getcwd() encoder = {} decoder = {} class Ui_MainWindow(object): def setupUi(self, MainWindow): MainWindow.setObjectName(&amp;#34;MainWindow&amp;#34;) MainWindow.resize(495, 328) self.centralwidget = QtWidgets.QWidget(MainWindow) self.centralwidget.setObjectName(&amp;#34;centralwidget&amp;#34;) self.textEdit = QtWidgets.QTextEdit(self.centralwidget) self.textEdit.setGeometry(QtCore.QRect(10, 10, 341, 291)) self.</description></item><item><title>File word retrieval and statistics using Python</title><link>https://lunarwhite.github.io/posts/assigns/ds-word-stat/</link><pubDate>Sat, 19 Dec 2020 19:48:51 +0000</pubDate><guid>https://lunarwhite.github.io/posts/assigns/ds-word-stat/</guid><description>1 要求 1.1 简介 给定一个文本文件，要求统计给定单词在文本中出现的总次数，并检索输出某个单词出现在文本中的行号、在该行中出现的次数以及位置。
1.2思路 可以分为主控菜单程序和另外三个部分实现：
零、主控菜单程序 (1) 头文件包含 (2) 菜单选项包括 建立文件 单词计数 单词定位 退出程序 (3) 选择1-4执行相应的操作，其他字符为非法 一、建立一个文本文件，文件名由用户用键盘输入 实现过程 (1) 定义一个串变量 (2) 定义文本文件 (3) 输入文件名，打开该文件 (4) 循环读入文本行，写入文本文件 (5) 关闭文件 二、给定单词计数，输入一个不含空格的单词，统计输出该单词在文本中的出现次数 实现设计 要用到模式匹配算法，逐行扫描文本文件。匹配一个，计数器加1，直到整个文件扫描结束；然后输出单词出现的次数 朴素模式匹配算法的基本思路是将给定字串与主串从第一个字符开始比较，找到首次与子串完全匹配的子串为止，并记住该位置。 但为了实现统计子串出现的个数，不仅需要从主串的第一个字符位置开始比较，而且需要从主串的任一位置检索匹配字符串。 实现过程 (1) 输入要检索的文本文件名，打开相应的文件 (2) 输入要检索统计的单词 (3) 循环读文本文件，读入一行，将其送入定义好的串中，并求该串的实际长度，调用串匹配函数进行计数 (4) 关闭文件，输出统计结果 三、检索给定单词，输入一个单词，检索并输出该单词所在的行号、该行中出现的次数以及在该行中的相应位置。 实现设计 同上一个设计类似，但是要相对复杂一些 实现过程 (1) 输入要检索的文本文件名，打开相应的文件 (2) 输入要检索统计的单词 (3) 循环读文本文件，读入一行，将其送入定义好的串中，并求该串的实际长度。行计数器置初值0，调用串匹配函数进行计数。如果 行单词计数器!</description></item></channel></rss>